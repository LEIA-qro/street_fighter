{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Street Fighter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing gym and retro.\n",
    "- Loading the ROM file.\n",
    "- Analysing the game space.\n",
    "- Testing the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import gym, retro\n",
    "\n",
    "import time  # For slowing down fights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n",
      "Retro version: 0.8.0\n",
      "Gym version: 0.21.0\n"
     ]
    }
   ],
   "source": [
    "# Optional: Check versions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Retro version: {retro.__version__}\")\n",
    "print(f\"Gym version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Check retro-compatible rom games\n",
    "\n",
    "# retro.data.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start game environment\n",
    "env = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis')\n",
    "\n",
    "\n",
    "# Closes game environment since we can only run one at a time\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0], dtype=int8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample the actions available - MultiBinary\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 12,  33,  62],\n",
       "        [ 67,  31,  35],\n",
       "        [255, 134,  69],\n",
       "        ...,\n",
       "        [166, 107, 227],\n",
       "        [ 58, 193, 154],\n",
       "        [143, 233,  16]],\n",
       "\n",
       "       [[191,  69, 139],\n",
       "        [189,  28, 193],\n",
       "        [131, 173, 113],\n",
       "        ...,\n",
       "        [ 88, 158, 100],\n",
       "        [233, 180, 228],\n",
       "        [218, 187,  57]],\n",
       "\n",
       "       [[ 72, 250, 122],\n",
       "        [ 63,  46,  97],\n",
       "        [208,  65, 171],\n",
       "        ...,\n",
       "        [156, 126,  69],\n",
       "        [ 61, 213, 171],\n",
       "        [130,  72, 231]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[130, 145,  36],\n",
       "        [ 33,  64,  70],\n",
       "        [ 74,  98,  69],\n",
       "        ...,\n",
       "        [249, 196,  38],\n",
       "        [166, 178,  89],\n",
       "        [227,  96, 102]],\n",
       "\n",
       "       [[ 76, 105, 197],\n",
       "        [ 48,   3, 117],\n",
       "        [ 52, 106, 228],\n",
       "        ...,\n",
       "        [ 37,  97, 207],\n",
       "        [230, 225, 223],\n",
       "        [205,  34, 105]],\n",
       "\n",
       "       [[ 36,  44, 255],\n",
       "        [141, 144, 200],\n",
       "        [ 86, 188,  69],\n",
       "        ...,\n",
       "        [208, 133, 206],\n",
       "        [215,  14, 254],\n",
       "        [ 38, 206,   2]]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample the observation space\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Render environment\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# We take random actions inside the environment\u001b[39;00m\n\u001b[0;32m     22\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n",
      "File \u001b[1;32mc:\\Code\\Street Fighter\\venv\\lib\\site-packages\\retro\\retro_env.py:230\u001b[0m, in \u001b[0;36mRetroEnv.render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic_control\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrendering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleImageViewer\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m SimpleImageViewer()\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39misopen\n",
      "File \u001b[1;32mc:\\Code\\Street Fighter\\venv\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:441\u001b[0m, in \u001b[0;36mSimpleImageViewer.imshow\u001b[1;34m(self, arr)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed in an image with the wrong number shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m image \u001b[38;5;241m=\u001b[39m pyglet\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mImageData(\n\u001b[0;32m    439\u001b[0m     arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr\u001b[38;5;241m.\u001b[39mtobytes(), pitch\u001b[38;5;241m=\u001b[39marr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    440\u001b[0m )\n\u001b[1;32m--> 441\u001b[0m texture \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_texture\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m gl\u001b[38;5;241m.\u001b[39mglTexParameteri(gl\u001b[38;5;241m.\u001b[39mGL_TEXTURE_2D, gl\u001b[38;5;241m.\u001b[39mGL_TEXTURE_MAG_FILTER, gl\u001b[38;5;241m.\u001b[39mGL_NEAREST)\n\u001b[0;32m    443\u001b[0m texture\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth\n",
      "File \u001b[1;32mc:\\Code\\Street Fighter\\venv\\lib\\site-packages\\pyglet\\image\\__init__.py:835\u001b[0m, in \u001b[0;36mImageData.get_texture\u001b[1;34m(self, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_texture\u001b[39m(\u001b[38;5;28mself\u001b[39m, rectangle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, force_rectangle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_texture \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    834\u001b[0m             (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_texture\u001b[38;5;241m.\u001b[39m_is_rectangle \u001b[38;5;129;01mand\u001b[39;00m force_rectangle)):\n\u001b[1;32m--> 835\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_texture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_texture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTexture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrectangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_rectangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_texture\n",
      "File \u001b[1;32mc:\\Code\\Street Fighter\\venv\\lib\\site-packages\\pyglet\\image\\__init__.py:821\u001b[0m, in \u001b[0;36mImageData.create_texture\u001b[1;34m(self, cls, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a texture containing this image.\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \n\u001b[0;32m    800\u001b[0m \u001b[38;5;124;03mIf the image's dimensions are not powers of 2, a TextureRegion of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;124;03m:rtype: cls or cls.region_class\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    820\u001b[0m internalformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_internalformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat)\n\u001b[1;32m--> 821\u001b[0m texture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternalformat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrectangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_rectangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_x \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_y:\n\u001b[0;32m    824\u001b[0m     texture\u001b[38;5;241m.\u001b[39manchor_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_x\n",
      "File \u001b[1;32mc:\\Code\\Street Fighter\\venv\\lib\\site-packages\\pyglet\\image\\__init__.py:1467\u001b[0m, in \u001b[0;36mTexture.create\u001b[1;34m(cls, width, height, internalformat, rectangle, force_rectangle, min_filter, mag_filter)\u001b[0m\n\u001b[0;32m   1465\u001b[0m glGenTextures(\u001b[38;5;241m1\u001b[39m, byref(\u001b[38;5;28mid\u001b[39m))\n\u001b[0;32m   1466\u001b[0m glBindTexture(target, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m-> 1467\u001b[0m \u001b[43mglTexParameteri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGL_TEXTURE_MIN_FILTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1468\u001b[0m glTexParameteri(target, GL_TEXTURE_MAG_FILTER, mag_filter)\n\u001b[0;32m   1470\u001b[0m blank \u001b[38;5;241m=\u001b[39m (GLubyte \u001b[38;5;241m*\u001b[39m (texture_width \u001b[38;5;241m*\u001b[39m texture_height \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m))()\n",
      "File \u001b[1;32mc:\\Code\\Street Fighter\\venv\\lib\\site-packages\\pyglet\\gl\\lib.py:87\u001b[0m, in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGLException\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merrcheck\u001b[39m(result, func, arguments):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _debug_gl_trace:\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test to see everything working\n",
    "\n",
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "\n",
    "# Flag to false\n",
    "done = False\n",
    "\n",
    "# We only play one game\n",
    "for game in range(1):\n",
    "\n",
    "    # If game is not over.\n",
    "    while not done:\n",
    "        if done:\n",
    "            # We reset the game\n",
    "            obs = env.reset()\n",
    "\n",
    "        # Render environment\n",
    "        env.render()\n",
    "\n",
    "        # We take random actions inside the environment\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "        # We slow down the renders so they are watchable\n",
    "        time.sleep(0)\n",
    "\n",
    "        # We print the reward\n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Once the testing is finished we close the environment and see what happened.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m----> 4\u001b[0m \u001b[43minfo\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "# Once the testing is finished we close the environment and see what happened.\n",
    "\n",
    "env.close()\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation preprocessing:\n",
    "1. Calculate change in pixels to capture movement (frame delta).\n",
    "2. Increase game efficiency by grayscaling and reshaping frames from 200x256x3 to 84x84x1  (153,600 pixels vs 7,056) for faster training.\n",
    "\n",
    "Action preprocessing:\n",
    "1. Filtering actions (parameters).\n",
    "2. Redefine reward functions based on score (possibility of basing it on enemy_health and health)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: 'opencv-python,'\n",
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Code\\Street Fighter\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install open cv\n",
    "%pip install opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries necessary for data preprocessing.\n",
    "\n",
    "from gym import Env  # Base environment class for a wrapper\n",
    "from gym.spaces import MultiBinary, Box  # Ensure we pick the correct action space type. (Space shapes for the environment)\n",
    "\n",
    "import numpy as np  # To calculate frame delta\n",
    "import cv2  # For grayscaling\n",
    "\n",
    "from matplotlib import pyplot as plt  # For plotting observation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom environment\n",
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "\n",
    "        # Inherit from our base environment\n",
    "        super().__init__()\n",
    "\n",
    "        # Specify action and observation spaces\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)  # We create our observation space based on the new size and colors\n",
    "        self.action_space = MultiBinary(12)  # We replicate the base action environment\n",
    "\n",
    "        # Startup and instance the game\n",
    "        # The second parameter will limit actions to only valid ones.\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions = retro.Actions.FILTERED)\n",
    "\n",
    "    def reset(self):\n",
    "        # Return first frame, preprocess the frame, and define score back to 0.\n",
    "\n",
    "        self.previous_frame = np.zeros(self.game.observation_space.shape)\n",
    "\n",
    "        obs = self.game.reset()  # Will return our observation\n",
    "        obs = self.preprocess(obs)  # We preprocess the observation\n",
    "\n",
    "        self.health = 176\n",
    "        self.enemy_health = 176\n",
    "        \n",
    "        # Game delta = Current_frame - Previous_frame\n",
    "        # Preprocess\n",
    "        self.previous_frame = obs\n",
    "\n",
    "        # Attribute to hold delta score.\n",
    "        self.score = 0\n",
    "\n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        # Grayscale, and resize frame\n",
    "        \n",
    "        # Grayscaling\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Resizing\n",
    "        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        channel = np.reshape(resize, (84, 84, 1))  # We add the grayscale layer since its what gym expects\n",
    "\n",
    "        return channel\n",
    "\n",
    "    def step(self, action):\n",
    "        # We take a step, preprocess the observation, calculate frame delta and reshape the reward function\n",
    "\n",
    "        # Take a step\n",
    "        obs, reward, done, info = self.game.step(action)  # New step based on an action\n",
    "\n",
    "        obs = self.preprocess(obs)  # We preprocess the observation\n",
    "\n",
    "        # Frame delta\n",
    "\n",
    "        # We subtract the current one from the previous one and then we set the current as the last one.\n",
    "        frame_delta = obs  # - self.previous_frame\n",
    "        # self.previous_frame = obs\n",
    "\n",
    "        # Reshape the reward function based on relative score\n",
    "        # reward = info['score'] - self.score  # Current reward minus the previous score\n",
    "        # self.score = info['score']  # We set our score to the current score.\n",
    "        delta_enemy = self.enemy_health - info['enemy_health']\n",
    "        delta_self = info['health'] - self.health\n",
    "\n",
    "        reward = delta_enemy * 2 + delta_self\n",
    "\n",
    "\n",
    "       # Update values\n",
    "        self.health = info['health']\n",
    "        self.enemy_health = info['enemy_health'] \n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        # We render the game\n",
    "        self.game.render()\n",
    "\n",
    "    def close(self):\n",
    "        # We close the game\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We close any environment that could be open\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter()  # We instance the created class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiBinary(12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "500\n",
      "500\n",
      "1000\n",
      "500\n",
      "1500\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "300\n",
      "300\n",
      "400\n",
      "1000\n",
      "100\n",
      "1000\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test to see everything working\n",
    "\n",
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "\n",
    "# Flag to false\n",
    "done = False\n",
    "\n",
    "# We only play one game\n",
    "for game in range(1):\n",
    "\n",
    "    # If game is not over.\n",
    "    while not done:\n",
    "        if done:\n",
    "            # We reset the game\n",
    "            obs = env.reset()\n",
    "\n",
    "        # Render environment\n",
    "        env.render()\n",
    "\n",
    "        # We take random actions inside the environment\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "        # We slow down the renders so they are watchable\n",
    "        time.sleep(0)\n",
    "\n",
    "        # We print the reward\n",
    "        if reward > 0:\n",
    "            print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PyTorch, Stable Baselines3 and Optuna to get the model's best training parameters.\n",
    "\n",
    "For PPO (Proximal Policy Optimization) we will tune the following hyperparameters:\n",
    "- n_steps: batch size (frames in buffer)\n",
    "- gamma: discount rate for calculating returns\n",
    "- learning_rate: learning coefficient for optimizer\n",
    "- clip_range: clipping amount for advantage calculation\n",
    "- gae_lambda: advantages smoothing parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting torch==2.0.1+cu118\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp38-cp38-win_amd64.whl (2619.2 MB)\n",
      "Collecting torchvision==0.15.2+cu118\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp38-cp38-win_amd64.whl (4.9 MB)\n",
      "Collecting jinja2\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\code\\street fighter\\venv\\lib\\site-packages (from torch==2.0.1+cu118) (4.13.2)\n",
      "Collecting filelock\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting sympy\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Collecting networkx\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting requests\n",
      "  Downloading https://download.pytorch.org/whl/requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: numpy in c:\\code\\street fighter\\venv\\lib\\site-packages (from torchvision==0.15.2+cu118) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from torchvision==0.15.2+cu118) (10.4.0)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl (17 kB)\n",
      "Collecting networkx\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading https://download.pytorch.org/whl/charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading https://download.pytorch.org/whl/certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading https://download.pytorch.org/whl/urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading https://download.pytorch.org/whl/idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, MarkupSafe, urllib3, sympy, networkx, jinja2, idna, filelock, charset-normalizer, certifi, torch, requests, torchvision\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2022.12.7 charset-normalizer-2.1.1 filelock-3.13.1 idna-3.4 jinja2-3.1.4 mpmath-1.3.0 networkx-3.0 requests-2.28.1 sympy-1.13.3 torch-2.0.1+cu118 torchvision-0.15.2+cu118 urllib3-1.26.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Code\\Street Fighter\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.1.2+cu121 torchvision==0.16.2+cu121 torchaudio==2.1.2+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Baselines3: https://stable-baselines3.readthedocs.io/en/master/guide/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3==1.7.0Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Code\\Street Fighter\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading stable_baselines3-1.7.0-py3-none-any.whl (171 kB)\n",
      "Requirement already satisfied: numpy in c:\\code\\street fighter\\venv\\lib\\site-packages (from stable-baselines3==1.7.0) (1.24.4)\n",
      "Requirement already satisfied: gym==0.21 in c:\\code\\street fighter\\venv\\lib\\site-packages (from stable-baselines3==1.7.0) (0.21.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\code\\street fighter\\venv\\lib\\site-packages (from stable-baselines3==1.7.0) (3.1.1)\n",
      "Requirement already satisfied: matplotlib in c:\\code\\street fighter\\venv\\lib\\site-packages (from stable-baselines3==1.7.0) (3.7.5)\n",
      "Collecting importlib-metadata~=4.13\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pandas in c:\\code\\street fighter\\venv\\lib\\site-packages (from stable-baselines3==1.7.0) (2.0.3)\n",
      "Requirement already satisfied: torch>=1.11 in c:\\code\\street fighter\\venv\\lib\\site-packages (from stable-baselines3==1.7.0) (2.0.1+cu118)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\code\\street fighter\\venv\\lib\\site-packages (from importlib-metadata~=4.13->stable-baselines3==1.7.0) (3.20.2)\n",
      "Requirement already satisfied: sympy in c:\\code\\street fighter\\venv\\lib\\site-packages (from torch>=1.11->stable-baselines3==1.7.0) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\code\\street fighter\\venv\\lib\\site-packages (from torch>=1.11->stable-baselines3==1.7.0) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\code\\street fighter\\venv\\lib\\site-packages (from torch>=1.11->stable-baselines3==1.7.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\code\\street fighter\\venv\\lib\\site-packages (from torch>=1.11->stable-baselines3==1.7.0) (3.1.4)\n",
      "Requirement already satisfied: filelock in c:\\code\\street fighter\\venv\\lib\\site-packages (from torch>=1.11->stable-baselines3==1.7.0) (3.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from jinja2->torch>=1.11->stable-baselines3==1.7.0) (2.1.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from matplotlib->stable-baselines3==1.7.0) (4.57.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from matplotlib->stable-baselines3==1.7.0) (6.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from matplotlib->stable-baselines3==1.7.0) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\code\\street fighter\\venv\\lib\\site-packages (from matplotlib->stable-baselines3==1.7.0) (3.1.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from matplotlib->stable-baselines3==1.7.0) (25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\code\\street fighter\\venv\\lib\\site-packages (from matplotlib->stable-baselines3==1.7.0) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\code\\street fighter\\venv\\lib\\site-packages (from matplotlib->stable-baselines3==1.7.0) (1.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\code\\street fighter\\venv\\lib\\site-packages (from matplotlib->stable-baselines3==1.7.0) (1.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\code\\street fighter\\venv\\lib\\site-packages (from matplotlib->stable-baselines3==1.7.0) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\code\\street fighter\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==1.7.0) (1.17.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\code\\street fighter\\venv\\lib\\site-packages (from pandas->stable-baselines3==1.7.0) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\code\\street fighter\\venv\\lib\\site-packages (from pandas->stable-baselines3==1.7.0) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from sympy->torch>=1.11->stable-baselines3==1.7.0) (1.3.0)\n",
      "Installing collected packages: importlib-metadata, stable-baselines3\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 8.5.0\n",
      "    Uninstalling importlib-metadata-8.5.0:\n",
      "      Successfully uninstalled importlib-metadata-8.5.0\n",
      "  Attempting uninstall: stable-baselines3\n",
      "    Found existing installation: stable-baselines3 2.4.1\n",
      "    Uninstalling stable-baselines3-2.4.1:\n",
      "      Successfully uninstalled stable-baselines3-2.4.1\n",
      "Successfully installed importlib-metadata-4.13.0 stable-baselines3-1.7.0\n"
     ]
    }
   ],
   "source": [
    "%pip install stable-baselines3==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna: https://optuna.org/#installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "Requirement already satisfied: tqdm in c:\\code\\street fighter\\venv\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Collecting PyYAML\n",
      "  Downloading pyyaml-6.0.3.tar.gz (130 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
      "Collecting sqlalchemy>=1.4.2\n",
      "  Downloading sqlalchemy-2.0.43-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: numpy in c:\\code\\street fighter\\venv\\lib\\site-packages (from optuna) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from optuna) (25.0)\n",
      "Collecting Mako\n",
      "  Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: importlib-resources in c:\\code\\street fighter\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (6.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\code\\street fighter\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\code\\street fighter\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (8.5.0)\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.1.1-cp38-cp38-win_amd64.whl (298 kB)\n",
      "Requirement already satisfied: colorama in c:\\code\\street fighter\\venv\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\code\\street fighter\\venv\\lib\\site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\code\\street fighter\\venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Building wheels for collected packages: PyYAML\n",
      "  Building wheel for PyYAML (PEP 517): started\n",
      "  Building wheel for PyYAML (PEP 517): finished with status 'done'\n",
      "  Created wheel for PyYAML: filename=PyYAML-6.0.3-cp38-cp38-win_amd64.whl size=45380 sha256=252c5234634e15e50543d1793d271f5c6474ca5cb34a3c84e2f917e6d1062dc8\n",
      "  Stored in directory: c:\\users\\sss\\appdata\\local\\pip\\cache\\wheels\\60\\9b\\bc\\8c54c7c157a008210f4d6ddfce9e9c5274162b18c5832f8e24\n",
      "Successfully built PyYAML\n",
      "Installing collected packages: greenlet, sqlalchemy, Mako, PyYAML, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.10 PyYAML-6.0.3 alembic-1.14.1 colorlog-6.9.0 greenlet-3.1.1 optuna-4.5.0 sqlalchemy-2.0.43\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Code\\Street Fighter\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shimmy>=2.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: gymnasium>=1.0.0a1 in c:\\code\\street fighter\\venv\\lib\\site-packages (from shimmy>=2.0) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from shimmy>=2.0) (1.24.4)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\code\\street fighter\\venv\\lib\\site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.13.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\code\\street fighter\\venv\\lib\\site-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\code\\street fighter\\venv\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium>=1.0.0a1->shimmy>=2.0) (3.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Code\\Street Fighter\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"shimmy>=2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Street Fighter\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna  # Importing the optimization framework that allows to both train and tune at the same time\n",
    "import torch\n",
    "import os  # For exporting the model\n",
    "from stable_baselines3 import PPO  # PPO algorithm for RL\n",
    "from stable_baselines3.common.evaluation import evaluate_policy  # Metric calculation of agent performance\n",
    "from stable_baselines3.common.monitor import Monitor  # SB3 Monitor for logging\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack  # Vec wrappers to vectorize and frame stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories where saved optimization models are going to be saved\n",
    "\n",
    "LOG_DIR = './logs/'  # SB3 has the ability to log out to a support log\n",
    "OPT_DIR = './opt/'  # Location to save every single model after every try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter function to return test hyperparameters - define the objective function\n",
    "\n",
    "def optimize_ppo(trial):  # i.e. objective\n",
    "    return {\n",
    "        # Ranges of possible values that will be optimized\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048, 8192, step=64),  # SB3 requires  the range to be a multiple of 64\n",
    "        'gamma': trial.suggest_loguniform('gamma', 0.8, 0.999),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range': trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', 0.8, 0.99),\n",
    "    }\n",
    "\n",
    "# When we train we will get a set of best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter function to run a training loop and return mean \n",
    "device = torch.device(\"cpu\")  # Fuerza CPU\n",
    "\n",
    "def optimize_agent(trial):\n",
    "    # A try - except section can prevent the model from breaking mid-training\n",
    "\n",
    "    model_params = optimize_ppo(trial)  # Variable where we store the parameters from the previous function\n",
    "\n",
    "    # Create environment\n",
    "    env = StreetFighter()\n",
    "    env = Monitor(env, LOG_DIR)  # We specify the location where monitor values will be exported to\n",
    "    env = DummyVecEnv([lambda: env])  # We wrap the environment on a DummyVec\n",
    "    env = VecFrameStack(env, 4, channels_order='last')  # We will stack 4 different frames\n",
    "\n",
    "    # Create training algorithm\n",
    "    # model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)  # We unpack the model parameters obtained from the tuner and pass them to the PPO model\n",
    "    model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, device=device, **model_params)\n",
    "    model.learn(total_timesteps=30000)  # We train the model. Longer timesteps means a better model, but also a longer training time. 100k is good, 30k is quick but inaccurate\n",
    "    \n",
    "    # Evaluate model\n",
    "    mean_reward = evaluate_policy(model, env, n_eval_episodes=5)  # We unpack the results obtained from evaluate policy. We will evaluate the model on 5 different games (more == better)\n",
    "    env.close()\n",
    "\n",
    "    SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "    model.save(SAVE_PATH)  # We save all models to get the best one\n",
    "\n",
    "    # We have to give optuna a value it expects, so if its a tuple we return only an int\n",
    "    if isinstance(mean_reward, (tuple, list)):\n",
    "        mean_reward = mean_reward[0]\n",
    "\n",
    "    return mean_reward \n",
    "    try:\n",
    "        pass\n",
    "\n",
    "    except Exception as e:\n",
    "        return -1000  # Model did not work, we resume training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-26 11:52:05,195] A new study created in memory with name: no-name-bcaa0e69-136c-4310-8d92-a314a6d3bf32\n",
      "C:\\Users\\SSS\\AppData\\Local\\Temp\\ipykernel_8404\\2097929532.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'gamma': trial.suggest_loguniform('gamma', 0.8, 0.999),\n",
      "C:\\Users\\SSS\\AppData\\Local\\Temp\\ipykernel_8404\\2097929532.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
      "C:\\Users\\SSS\\AppData\\Local\\Temp\\ipykernel_8404\\2097929532.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'clip_range': trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
      "C:\\Users\\SSS\\AppData\\Local\\Temp\\ipykernel_8404\\2097929532.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'gae_lambda': trial.suggest_uniform('gae_lambda', 0.8, 0.99),\n",
      "c:\\Code\\Street Fighter\\venv\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:278: UserWarning: Path 'opt' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n",
      "[I 2025-09-26 11:56:59,586] Trial 0 finished with value: 2500.0 and parameters: {'n_steps': 5440, 'gamma': 0.943834098928334, 'learning_rate': 2.918781443077117e-05, 'clip_range': 0.28796621761888674, 'gae_lambda': 0.8063464298483354}. Best is trial 0 with value: 2500.0.\n",
      "[I 2025-09-26 12:01:29,439] Trial 1 finished with value: 2500.0 and parameters: {'n_steps': 5504, 'gamma': 0.9176852395668519, 'learning_rate': 2.2914397555619692e-05, 'clip_range': 0.28873852140734724, 'gae_lambda': 0.9128392193844321}. Best is trial 0 with value: 2500.0.\n",
      "[I 2025-09-26 12:06:03,004] Trial 2 finished with value: 1100.0 and parameters: {'n_steps': 3200, 'gamma': 0.8139521151556859, 'learning_rate': 3.0455592648426806e-05, 'clip_range': 0.1139746809716472, 'gae_lambda': 0.8457941146793637}. Best is trial 0 with value: 2500.0.\n",
      "[I 2025-09-26 12:11:05,127] Trial 3 finished with value: 3400.0 and parameters: {'n_steps': 7104, 'gamma': 0.9427802452995265, 'learning_rate': 3.893934096840782e-05, 'clip_range': 0.11626752086014813, 'gae_lambda': 0.9199219928178809}. Best is trial 3 with value: 3400.0.\n",
      "[I 2025-09-26 12:16:02,889] Trial 4 finished with value: 2900.0 and parameters: {'n_steps': 5824, 'gamma': 0.9821764863885649, 'learning_rate': 5.845747382153418e-05, 'clip_range': 0.2583995106940593, 'gae_lambda': 0.8825603330848883}. Best is trial 3 with value: 3400.0.\n",
      "[I 2025-09-26 12:20:39,183] Trial 5 finished with value: 0.0 and parameters: {'n_steps': 3072, 'gamma': 0.9943067867985889, 'learning_rate': 9.34183308627991e-05, 'clip_range': 0.1419372468146163, 'gae_lambda': 0.863135000013458}. Best is trial 3 with value: 3400.0.\n",
      "[I 2025-09-26 12:26:20,748] Trial 6 finished with value: 1300.0 and parameters: {'n_steps': 8192, 'gamma': 0.9790420388312194, 'learning_rate': 1.349708401089439e-05, 'clip_range': 0.22462879295964844, 'gae_lambda': 0.8433338972488237}. Best is trial 3 with value: 3400.0.\n",
      "[I 2025-09-26 12:31:11,611] Trial 7 finished with value: 3500.0 and parameters: {'n_steps': 7808, 'gamma': 0.8272439796475773, 'learning_rate': 5.5591559378224225e-05, 'clip_range': 0.3058866148896639, 'gae_lambda': 0.8393585233530592}. Best is trial 7 with value: 3500.0.\n",
      "[I 2025-09-26 12:35:56,085] Trial 8 finished with value: 4100.0 and parameters: {'n_steps': 5056, 'gamma': 0.8959654501300689, 'learning_rate': 2.9754004787916876e-05, 'clip_range': 0.18000274760964202, 'gae_lambda': 0.9275787506240445}. Best is trial 8 with value: 4100.0.\n",
      "[I 2025-09-26 12:40:27,353] Trial 9 finished with value: 4400.0 and parameters: {'n_steps': 7360, 'gamma': 0.8152363058260785, 'learning_rate': 7.627838127572661e-05, 'clip_range': 0.3129437660456802, 'gae_lambda': 0.9822166342841949}. Best is trial 9 with value: 4400.0.\n"
     ]
    }
   ],
   "source": [
    "# Tuning\n",
    "\n",
    "study = optuna.create_study(direction='maximize')  # We create the experiment / study that seeks to maximize the mean reward\n",
    "study.optimize(optimize_agent, n_trials=10, n_jobs=1)  # We optimize the study based on the agent created, and how many sets we will set. 10 is good for testing, 100+ is recommended for a good model\n",
    "\n",
    "# NOTE: Using 100k timesteps on the model and 100 trials can take a long time to train (depending on the strength of the gpu from a few hours to a couple of days)\n",
    "\n",
    "# If we wanted to speed things up whilst keeping accuracy, we could raise n_jobs, however retro does not support more than one environment at once. We can fix\n",
    "# this by using retrowrapper: https://github.com/MaxStrange/retrowrapper. This allows for multiple instances at once which exponentially speeds trainig up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_steps': 7360,\n",
       " 'gamma': 0.8152363058260785,\n",
       " 'learning_rate': 7.627838127572661e-05,\n",
       " 'clip_range': 0.3129437660456802,\n",
       " 'gae_lambda': 0.9822166342841949}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pass it through a model we use\n",
    "# model = PPO.load(os.path.join(OPT_DIR, 'trial_0_best_model.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_video = {\n",
    "    'n_steps': 7488,\n",
    "    'gamma': 0.9085173842732223,\n",
    "    'learning_rate': 5e-7,\n",
    "    'clip_range': 0.39105070719865653,\n",
    "    'gae_lambda': 0.8376637411768156\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Callback\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)  # We will save the model every 10k steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = study.best_params\n",
    "model_params['learning_rate'] = 5e-7  # Might improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "# env.close()\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, device=device, **model_params)\n",
    "\n",
    "model.load(os.path.join(OPT_DIR, 'trial_24_best_model.zip'))  # We reload previous weights from HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/PPO_11\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 461  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 7360 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.46e+03     |\n",
      "|    ep_rew_mean          | 1.25e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 263          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 14720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.018814e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.313        |\n",
      "|    entropy_loss         | -8.32        |\n",
      "|    explained_variance   | 2.4e-05      |\n",
      "|    learning_rate        | 5e-07        |\n",
      "|    loss                 | 50.5         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -4.21e-05    |\n",
      "|    value_loss           | 7.92e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.5e+03      |\n",
      "|    ep_rew_mean          | 1.66e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 238          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 22080        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.381147e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.313        |\n",
      "|    entropy_loss         | -8.32        |\n",
      "|    explained_variance   | -0.000174    |\n",
      "|    learning_rate        | 5e-07        |\n",
      "|    loss                 | 1.78e+04     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -6.79e-05    |\n",
      "|    value_loss           | 5.68e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.34e+03      |\n",
      "|    ep_rew_mean          | 1.6e+04       |\n",
      "| time/                   |               |\n",
      "|    fps                  | 226           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 130           |\n",
      "|    total_timesteps      | 29440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4378518e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.313         |\n",
      "|    entropy_loss         | -8.32         |\n",
      "|    explained_variance   | -0.000613     |\n",
      "|    learning_rate        | 5e-07         |\n",
      "|    loss                 | 370           |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -3.05e-05     |\n",
      "|    value_loss           | 1.01e+04      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 7.56e+03     |\n",
      "|    ep_rew_mean          | 1.34e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 169          |\n",
      "|    total_timesteps      | 36800        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.062062e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.313        |\n",
      "|    entropy_loss         | -8.32        |\n",
      "|    explained_variance   | 0.000734     |\n",
      "|    learning_rate        | 5e-07        |\n",
      "|    loss                 | 2.06e+03     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -4.11e-05    |\n",
      "|    value_loss           | 2.06e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.77e+03      |\n",
      "|    ep_rew_mean          | 1.72e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 210           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 209           |\n",
      "|    total_timesteps      | 44160         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.9630485e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.313         |\n",
      "|    entropy_loss         | -8.32         |\n",
      "|    explained_variance   | -0.000122     |\n",
      "|    learning_rate        | 5e-07         |\n",
      "|    loss                 | 424           |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -5.57e-05     |\n",
      "|    value_loss           | 5.86e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.77e+03     |\n",
      "|    ep_rew_mean          | 1.72e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 201          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 256          |\n",
      "|    total_timesteps      | 51520        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.378898e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.313        |\n",
      "|    entropy_loss         | -8.32        |\n",
      "|    explained_variance   | -0.00024     |\n",
      "|    learning_rate        | 5e-07        |\n",
      "|    loss                 | 172          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -6.34e-05    |\n",
      "|    value_loss           | 5.17e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.93e+03      |\n",
      "|    ep_rew_mean          | 1.71e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 195           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 301           |\n",
      "|    total_timesteps      | 58880         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9042436e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.313         |\n",
      "|    entropy_loss         | -8.32         |\n",
      "|    explained_variance   | -0.000565     |\n",
      "|    learning_rate        | 5e-07         |\n",
      "|    loss                 | 2.07e+04      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -7.96e-05     |\n",
      "|    value_loss           | 9.02e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.93e+03     |\n",
      "|    ep_rew_mean          | 1.71e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 194          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 340          |\n",
      "|    total_timesteps      | 66240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.085226e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.313        |\n",
      "|    entropy_loss         | -8.32        |\n",
      "|    explained_variance   | -0.000608    |\n",
      "|    learning_rate        | 5e-07        |\n",
      "|    loss                 | 47.9         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000121    |\n",
      "|    value_loss           | 7.57e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+04         |\n",
      "|    ep_rew_mean          | 2.42e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 189           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 387           |\n",
      "|    total_timesteps      | 73600         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3879427e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.313         |\n",
      "|    entropy_loss         | -8.32         |\n",
      "|    explained_variance   | -0.00031      |\n",
      "|    learning_rate        | 5e-07         |\n",
      "|    loss                 | 2.39e+05      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -4.89e-05     |\n",
      "|    value_loss           | 9.68e+04      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+04     |\n",
      "|    ep_rew_mean          | 2.41e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 440          |\n",
      "|    total_timesteps      | 80960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.365415e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.313        |\n",
      "|    entropy_loss         | -8.32        |\n",
      "|    explained_variance   | -0.00161     |\n",
      "|    learning_rate        | 5e-07        |\n",
      "|    loss                 | 324          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -5.7e-05     |\n",
      "|    value_loss           | 7.31e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.57e+03     |\n",
      "|    ep_rew_mean          | 2.19e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 180          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 490          |\n",
      "|    total_timesteps      | 88320        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.697238e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.313        |\n",
      "|    entropy_loss         | -8.32        |\n",
      "|    explained_variance   | 4.76e-05     |\n",
      "|    learning_rate        | 5e-07        |\n",
      "|    loss                 | 5.24e+03     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000191    |\n",
      "|    value_loss           | 7.75e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.57e+03     |\n",
      "|    ep_rew_mean          | 2.19e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 176          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 541          |\n",
      "|    total_timesteps      | 95680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.713426e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.313        |\n",
      "|    entropy_loss         | -8.32        |\n",
      "|    explained_variance   | 0.00106      |\n",
      "|    learning_rate        | 5e-07        |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000177    |\n",
      "|    value_loss           | 3.1e+03      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 9.65e+03      |\n",
      "|    ep_rew_mean          | 2.25e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 177           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 580           |\n",
      "|    total_timesteps      | 103040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8266102e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.313         |\n",
      "|    entropy_loss         | -8.32         |\n",
      "|    explained_variance   | -0.000398     |\n",
      "|    learning_rate        | 5e-07         |\n",
      "|    loss                 | 1.37e+03      |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.000103     |\n",
      "|    value_loss           | 3.08e+04      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x12a560d78e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.learn(total_timesteps=100000, callback=callback)  # Bigger is better, for example 5 million\n",
    "# model.learn(total_timesteps=5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing and Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load and visualize the training result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\Street Fighter\\venv\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: an integer is required (got type bytes)\n",
      "  warnings.warn(\n",
      "c:\\Code\\Street Fighter\\venv\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:166: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: an integer is required (got type bytes)\n",
      "  warnings.warn(\n",
      "c:\\Code\\Street Fighter\\venv\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:719: UserWarning: You are probably loading a model saved with SB3 < 1.7.0, we deactivated exact_match so you can save the model again to avoid issues in the future (see https://github.com/DLR-RM/stable-baselines3/issues/1233 for more info). Original error: Error(s) in loading state_dict for ActorCriticCnnPolicy:\n",
      "\tMissing key(s) in state_dict: \"pi_features_extractor.cnn.0.weight\", \"pi_features_extractor.cnn.0.bias\", \"pi_features_extractor.cnn.2.weight\", \"pi_features_extractor.cnn.2.bias\", \"pi_features_extractor.cnn.4.weight\", \"pi_features_extractor.cnn.4.bias\", \"pi_features_extractor.linear.0.weight\", \"pi_features_extractor.linear.0.bias\", \"vf_features_extractor.cnn.0.weight\", \"vf_features_extractor.cnn.0.bias\", \"vf_features_extractor.cnn.2.weight\", \"vf_features_extractor.cnn.2.bias\", \"vf_features_extractor.cnn.4.weight\", \"vf_features_extractor.cnn.4.bias\", \"vf_features_extractor.linear.0.weight\", \"vf_features_extractor.linear.0.bias\".  \n",
      "Note: the model should still work fine, this only a warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load('./train/best_model_5460000.zip', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, render=False, n_eval_episodes=5)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.]\n",
      "[18.]\n",
      "[74.]\n",
      "[110.]\n",
      "[70.]\n",
      "[22.]\n",
      "[57.]\n",
      "[62.]\n",
      "[56.]\n",
      "[58.]\n",
      "[14.]\n",
      "[72.]\n",
      "[12.]\n",
      "[74.]\n",
      "[96.]\n",
      "[12.]\n",
      "[52.]\n",
      "[119.]\n",
      "[52.]\n",
      "[52.]\n",
      "[50.]\n",
      "[52.]\n",
      "[12.]\n",
      "[70.]\n",
      "[65.]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test to see everything working\n",
    "\n",
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "\n",
    "# Flag to false\n",
    "done = False\n",
    "\n",
    "# We only play one game\n",
    "for game in range(1):\n",
    "\n",
    "    # If game is not over.\n",
    "    while not done:\n",
    "        if done:\n",
    "            # We reset the game\n",
    "            obs = env.reset()\n",
    "\n",
    "        # Render environment\n",
    "        env.render()\n",
    "\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # We slow down the renders so they are watchable\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        # We print the reward\n",
    "        if reward > 0:\n",
    "            print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
