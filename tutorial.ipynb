{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Street Fighter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing gym and retro.\n",
    "- Loading the ROM file.\n",
    "- Analysing the game space.\n",
    "- Testing the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python matplotlib\n",
    "%pip install torch==2.1.2+cu121 torchvision==0.16.2+cu121 torchaudio==2.1.2+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install stable-baselines3==1.7.0\n",
    "%pip install optuna\n",
    "%pip install \"shimmy>=2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym==0.21.0 gym-retro==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import gym, retro\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "# Libraries necessary for data preprocessing.\n",
    "\n",
    "from gym import Env  # Base environment class for a wrapper\n",
    "from gym.spaces import MultiBinary, Box  # Ensure we pick the correct action space type. (Space shapes for the environment)\n",
    "\n",
    "import numpy as np  # To calculate frame delta\n",
    "import cv2  # For grayscaling\n",
    "\n",
    "from matplotlib import pyplot as plt  # For plotting observation images\n",
    "\n",
    "\n",
    "# Libraries for training\n",
    "import optuna  # Optimization framework that allows to both train and tune at the same time\n",
    "from stable_baselines3 import PPO  # PPO algorithm for RL\n",
    "from stable_baselines3.common.evaluation import evaluate_policy  # Metric calculation of agent performance\n",
    "from stable_baselines3.common.monitor import Monitor  # SB3 Monitor for logging\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack  # Vec wrappers to vectorize and frame stack\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "# Optional: Check versions\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Retro version: {retro.__version__}\")\n",
    "print(f\"Gym version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro.data.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start game environment\n",
    "env = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the actions available - MultiBinary\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the observation space\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see everything working\n",
    "\n",
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "\n",
    "# Flag to false\n",
    "done = False\n",
    "\n",
    "# We only play one game\n",
    "for game in range(1):\n",
    "\n",
    "    # If game is not over.\n",
    "    while not done:\n",
    "        if done:\n",
    "            # We reset the game\n",
    "            obs = env.reset()\n",
    "\n",
    "        # Render environment\n",
    "        env.render()\n",
    "\n",
    "        # We take random actions inside the environment\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "        # We slow down the renders so they are watchable\n",
    "        time.sleep(0)\n",
    "\n",
    "        # We print the reward\n",
    "        print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the testing is finished we close the environment and see what happened.\n",
    "\n",
    "env.close()\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation preprocessing:\n",
    "1. Calculate change in pixels to capture movement (frame delta).\n",
    "2. Increase game efficiency by grayscaling and reshaping frames from 200x256x3 to 84x84x1  (153,600 pixels vs 7,056) for faster training.\n",
    "\n",
    "Action preprocessing:\n",
    "1. Filtering actions (parameters).\n",
    "2. Redefine reward functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "env_def"
    ]
   },
   "outputs": [],
   "source": [
    "# Create custom environment\n",
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "\n",
    "        # Inherit from our base environment\n",
    "        super().__init__()\n",
    "\n",
    "        # Specify action and observation spaces\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)  # We create our observation space based on the new size and colors\n",
    "        self.action_space = MultiBinary(12)  # We replicate the base action environment\n",
    "\n",
    "        # Startup and instance the game\n",
    "        # The second parameter will limit actions to only valid ones.\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions = retro.Actions.FILTERED)\n",
    "\n",
    "    def reset(self):\n",
    "        # Return first frame, preprocess the frame, and define score back to 0.\n",
    "\n",
    "        self.previous_frame = np.zeros(self.game.observation_space.shape)\n",
    "\n",
    "        obs = self.game.reset()  # Will return our observation\n",
    "        obs = self.preprocess(obs)  # We preprocess the observation\n",
    "\n",
    "        self.health = 176\n",
    "        self.enemy_health = 176\n",
    "        \n",
    "\n",
    "        # Attribute to hold delta score.\n",
    "        self.score = 0\n",
    "\n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        # Grayscale, and resize frame\n",
    "        \n",
    "        # Grayscaling\n",
    "        gray = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Resizing\n",
    "        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        channel = np.reshape(resize, (84, 84, 1))  # We add the grayscale layer since its what gym expects\n",
    "\n",
    "        return channel\n",
    "\n",
    "    def step(self, action):\n",
    "        # We take a step, preprocess the observation, calculate frame delta and reshape the reward function\n",
    "\n",
    "        # Take a step\n",
    "        obs, reward, done, info = self.game.step(action)  # New step based on an action\n",
    "\n",
    "        obs = self.preprocess(obs)  # We preprocess the observation\n",
    "\n",
    "        # Frame delta\n",
    "        frame_delta = obs\n",
    "\n",
    "        # Reshape the reward function based on relative score\n",
    "        delta_enemy = self.enemy_health - info['enemy_health']\n",
    "        delta_self = info['health'] - self.health\n",
    "\n",
    "        reward = delta_enemy * 2 + delta_self\n",
    "\n",
    "\n",
    "       # Update values\n",
    "        self.health = info['health']\n",
    "        self.enemy_health = info['enemy_health'] \n",
    "\n",
    "        return frame_delta, reward, done, info\n",
    "\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        # We render the game\n",
    "        self.game.render()\n",
    "\n",
    "    def close(self):\n",
    "        # We close the game\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We close any environment that could be open\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter()  # We instance the created class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see everything working\n",
    "\n",
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "\n",
    "# Flag to false\n",
    "done = False\n",
    "\n",
    "# We only play one game\n",
    "for game in range(1):\n",
    "\n",
    "    # If game is not over.\n",
    "    while not done:\n",
    "        if done:\n",
    "            # We reset the game\n",
    "            obs = env.reset()\n",
    "\n",
    "        # Render environment\n",
    "        env.render()\n",
    "\n",
    "        # We take random actions inside the environment\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "        # We slow down the renders so they are watchable\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        # We print the reward\n",
    "        if reward > 0:\n",
    "            print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PyTorch, Stable Baselines3 and Optuna to get the model's best training parameters.\n",
    "\n",
    "For PPO (Proximal Policy Optimization) we will tune the following hyperparameters:\n",
    "- n_steps: batch size (frames in buffer)\n",
    "- gamma: discount rate for calculating returns\n",
    "- learning_rate: learning coefficient for optimizer\n",
    "- clip_range: clipping amount for advantage calculation\n",
    "- gae_lambda: advantages smoothing parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Baselines3: https://stable-baselines3.readthedocs.io/en/master/guide/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna: https://optuna.org/#installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "logs"
    ]
   },
   "outputs": [],
   "source": [
    "# Directories where saved optimization models are going to be saved\n",
    "\n",
    "LOG_DIR = './logs/'  # SB3 has the ability to log out to a support log\n",
    "OPT_DIR = './opt/'  # Location to save every single model after every try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "optimize_ppo"
    ]
   },
   "outputs": [],
   "source": [
    "# Hyperparameter function to return test hyperparameters - define the objective function\n",
    "\n",
    "def optimize_ppo(trial):  # i.e. objective\n",
    "    return {\n",
    "        # Ranges of possible values that will be optimized\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048, 8192, step=64),  # SB3 requires  the range to be a multiple of 64\n",
    "        'gamma': trial.suggest_loguniform('gamma', 0.8, 0.999),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range': trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', 0.8, 0.99),\n",
    "    }\n",
    "\n",
    "# When we train we will get a set of best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "optimize_ppo_agent"
    ]
   },
   "outputs": [],
   "source": [
    "# Hyperparameter function to run a training loop and return mean \n",
    "\n",
    "eval_episodes = 5  # Number of times the model is evaluated. More = better.\n",
    "n_steps = 30000  # Number of steps we train the model for. More = better but also a longer training time. 100k is good, 30k is quick but inaccurate.\n",
    "\n",
    "def optimize_agent(trial):\n",
    "    # A try - except section can prevent the model from breaking mid-training\n",
    "    try:\n",
    "        model_params = optimize_ppo(trial)  # Variable where we store the parameters from the previous function\n",
    "\n",
    "        # Create environment\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)  # We specify the location where monitor values will be exported to\n",
    "        env = DummyVecEnv([lambda: env])  # We wrap the environment on a DummyVec\n",
    "        env = VecFrameStack(env, 4, channels_order='last')  # We will stack 4 different frames\n",
    "\n",
    "        # Create training algorithm\n",
    "        # model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)  # We unpack the model parameters obtained from the tuner and pass them to the PPO model\n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        model.learn(total_timesteps= n_steps)  # We train the model. Longer timesteps means a better model, but also a longer training time. 100k is good, 30k is quick but inaccurate.\n",
    "        \n",
    "        # Evaluate model\n",
    "        mean_reward = evaluate_policy(model, env, n_eval_episodes= eval_episodes)  # We unpack the results obtained from evaluate policy. We will evaluate the model on 5 different games (more == better)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)  # We save all models to get the best one\n",
    "\n",
    "        # We have to give optuna a value it expects, so if its a tuple we return only an int\n",
    "        if isinstance(mean_reward, (tuple, list)):\n",
    "            mean_reward = mean_reward[0]\n",
    "\n",
    "        return mean_reward \n",
    "\n",
    "    except Exception as e:\n",
    "        return -1000  # Model did not work, we resume training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "study = optuna.create_study(direction='maximize')  # We create the experiment / study that seeks to maximize the mean reward\n",
    "study.optimize(optimize_agent, n_trials=10, n_jobs=1)  # We optimize the study based on the agent created, and how many sets we will set. 10 is good for testing, 100+ is recommended for a good model\n",
    "\n",
    "# NOTE: Using 100k timesteps on the model and 100 trials can take a long time to train (depending on the strength of the gpu from a few hours to a couple of days)\n",
    "\n",
    "# If we wanted to speed things up whilst keeping accuracy, we could raise n_jobs, however retro does not support more than one environment at once. We can fix\n",
    "# this by using retrowrapper: https://github.com/MaxStrange/retrowrapper. This allows for multiple instances at once which exponentially speeds trainig up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pass it through a model we use\n",
    "# model = PPO.load(os.path.join(OPT_DIR, 'trial_0_best_model.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup_callback"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup Callback\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "logs"
    ]
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "callback"
    ]
   },
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)  # We will save the model every 10k steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "model"
    ]
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "# env.close()\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "\n",
    "model.load(os.path.join(OPT_DIR, 'trial_24_best_model.zip'))  # We reload previous weights from HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.learn(total_timesteps=100000, callback=callback)  # Bigger is better, for example 5-20 million.\n",
    "# model.learn(total_timesteps=5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing and Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load and visualize the training result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "model_load"
    ]
   },
   "outputs": [],
   "source": [
    "model = PPO.load('./train/best_model_10000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, render=False, n_eval_episodes=5)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "# Test to see everything working\n",
    "\n",
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "\n",
    "# Flag to false\n",
    "done = False\n",
    "\n",
    "# We only play one game\n",
    "for game in range(2):\n",
    "\n",
    "    # If game is not over.\n",
    "    while not done:\n",
    "        if done:\n",
    "            # We reset the game\n",
    "            obs = env.reset()\n",
    "\n",
    "        # Render environment\n",
    "        env.render()\n",
    "\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # We slow down the renders so they are watchable\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        # We print the reward\n",
    "        if reward > 0:\n",
    "            print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
